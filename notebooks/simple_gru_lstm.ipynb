{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=0.2860 ,std= 0.3530)])\n",
    "dataset = FashionMNIST(root='data/', download=True,\n",
    "transform=transforms)\n",
    "train, val = random_split(dataset, [55000, 5000])\n",
    "train =  DataLoader(train, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)\n",
    "val = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.LSTM(input_size,hidden_size,num_layers,batch_first = False)\n",
    "        self.fc1 = nn.Linear(hidden_size,output_size)\n",
    "    def forward(self,x):\n",
    "        x,_= self.rnn(x) # x = (seq_len,batch_size,input_size)\n",
    "        x = x[-1,:,:]\n",
    "        x = self.fc1(x)\n",
    "        return x.softmax(dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBIH6YnYdfOO"
   },
   "source": [
    "This [paper](https://arxiv.org/pdf/1508.02774.pdf) has been used as reference for obtaining the hidden size of the LSTM and the learning rate. Basically, it says that the LSTM is really sensible to the learnign rate and the hidden size; yet other parameters as the batch size and the momentum doest not really affect the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1_000\n",
    "model = SimpleLSTM(1,512,10)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "  loss_list = np.array([])\n",
    "  for i, batch_idex in enumerate(train):\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch_idex\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    x = x.permute(1,0,2,3)\n",
    "    x = x.reshape(1,BATCH_SIZE,28*28)\n",
    "    x = x.permute(2,1,0)\n",
    "    y = F.one_hot(y, num_classes=10).view(10,BATCH_SIZE)\n",
    "    y = y.float()\n",
    "    y = y.permute(1,0)\n",
    "    pred = model(x) # 64,784,1\n",
    "    loss = criterion(pred,y)\n",
    "    loss_list = np.append(loss_list,loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  \n",
    "  print(f\"Epoch {epoch} de {EPOCHS}\")\n",
    "  print(f\"Loss: {loss_list.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitSimpleLTSM(pl.LightningModule):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers = 1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.RNN = SimpleLSTM(input_size=input_size,hidden_size=hidden_size,output_size=output_size,num_layers=num_layers)\n",
    "    \n",
    "    def hot_encode(self,y, num_classes=10):\n",
    "        \"\"\"\n",
    "        One hot encode an int\n",
    "        \"\"\"\n",
    "        y = F.one_hot(y, num_classes=10).view(10,BATCH_SIZE)\n",
    "        return y.float()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.permute(1,0,2,3)\n",
    "        x = x.reshape(1,BATCH_SIZE,28*28)\n",
    "        x = x.permute(2,1,0)\n",
    "        y = self.hot_encode(y,10).view(10,BATCH_SIZE)\n",
    "        y = y.permute(1,0)\n",
    "        pred = self.RNN(x)\n",
    "        loss = nn.CrossEntropyLoss()(pred,y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        pass\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04c55434ed2b08bb5bf53ae5f55862aa0e3be3e7e24f24482bddca5922bab6dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
